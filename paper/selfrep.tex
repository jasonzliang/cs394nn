\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[protrusion=true,
            expansion=true]{microtype}
%\usepackage{amssymb}
%\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{color}
\usepackage[usenames,
            dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{kpfonts}
\usepackage[T1]{fontenc}
\usepackage{setspace}
\singlespacing
%\onehalfspacing
%\doublespacing

\newcommand{\term}[1]{\emph{#1}}

\begin{document}

\title{Programs that Program}

\author{Keenan Breik, Jason Liang}
\date{}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{intro}

Evolutionary computation allows computers
to automatically solve problems
that can be cast as optimization problems.
Until now, implementations have been
in large part hand designed.
Designers struggle to find meaningful constructs
and optimal tunings that allow evolutionary compuatation
to efficiently and effectively solve problems.
%to efficient, effective evolutionary computation.
%This design is part of solving target problems,
But once it is factored into the cost
of solving that problem,
that struggle is unjustifiable.

We propose to allow computers
to automatically design such implementations
by using evolutionary computation itself.
One way of doing this is to set up
an existing implementation,
such as genetic programming,
to search for a better one.
But by using a fixed implementation,
we must wait until the the process completes
to get a usable result,
and we still must hand-pick that fixed implementation.
An alternative is to start with an implementation
and allow it to modify itself.
Thus it would improve over time
and also improve at improving itself.

Two essential elements of an implementation
that must be established
are the candidate solution representation
and the population operators.
The representation is a form for storing a candidate
along with a decoding method
to extract it from storage.
To be general,
this representation can be an arbitrary program.
The population operators
generate new individuals from the current population.
To be general,
these operators can be arbitrary programs.

In this paper,
we demonstrate that neural networks
can generate other meaningful neural networks.
Since arbitrary neural networks are computationally powerful,%
\cite{sperduti1997netpower}
we can use them as candidate solution representations
and also as population operators.

%[explain how programs that program
%would give us this]
%
%To do so, we desire programs
%that write other programs
%and thereby explore a search space.
%
%In this paper,
%we demonstrate that neural networks
%can generate other meaningful neural networks.
%[Be clear. Elaborate.]

%[Explain why self-replication.]

The following is an overview of the rest of the paper:
In section \ref{problemstatement},
we discuss the general problem of generating networks.
In section \ref{feedforward},
we go over our methods
for constructing self-replicating feedforward neural networks.
In section \ref{results},
we discuss experimental results.
Finally in section \ref{conclusion},
we present conclusions and future work.

\section{Related Work}
\label{related}

Meta-optimization
has been used to tune implementations
of evolutionary computation.%
\cite{brest2006param}

Hyper-heuristics
have been used to avoid chosing a single fixed implementation.

Parameter-free, self-optimizing metaheuristic.

Gentic programming has been used
to automatically develop computer programs.
Evolutionary 

We draw on the notion of a CPPN (compositional
pattern-producing network).\cite{stanley2007cppn}
CPPNs, however, are meant to encode the structure
of a network, whereas we use them
for more general purpose information storage
and pattern generation.

Genetic programming?
Compositional pattern-producing networks.

\section{Decoding Networks}
\label{problemstatement}

A feedforward neural network $N$ encodes a function $f_N$
and thus is capable of storing information.
An arbitrary neural network $N$ encodes a program $P_N$
and thus is also capable of storing information.
One type of information a network may store
is an encoding of a second neural network $D(N)$.
But just as the meaning of a word
depends on the language being spoken,
so does the network encoded by $N$
depend on the semantics chosen.

To establish these semantics,
we may define what $N$ should be
in order to encode a given network $D(N)$.
This is the problem of \term{encoding}.
But if we want a network $N$
to generate another network $D(N)$,
then $D(N)$ is not given.
It is generated.
Solving the problem of encoding does not help.

An alternative way to establish the semantics
is to define what network $D(N)$ is encoded
by a given network $N$.
This is the problem of \term{decoding},
and we focus on it.
Solving the problem of decoding
consists of defining $D$
and allows us to have neural networks
generate other neural networks.

In order to give a network $N$
the power to generate another network
in an interesting way,
we prefer the decoding function $D$
to be neither completely stable
nor completely chaotic.
Instead, we want it somewhere around the edge of chaos.%
\cite{langton1990edgechaos}%
\cite{bertschinger2004edgechaos}
This seems to pose a challenge,
but the function or program a neural network encodes
satisfies exactly this property.
So we can define $D(N)$ in terms of $f_N$ or $P_N$.
In other words, part of computing $D(N)$
can be applying $f_N$ or $P_N$.

We can consider the features of $D(N)$,
such as how its neurons connect,
to be stored at particular points
in $f_N$
or to be outputs of $P_N$.
To extract the features of $D(N)$,
we simply query $N$ at such points.

%We consider ? schemes.
%[x0 ... xn] [y0 ... yn] ---> w
%[L S t s] ---> [n n w]
%arbitrary network

%\section{Evolution}
%algorithms. fitness functions.

\label{feedforward}

To create a self-replicating feedforward neural network,
we first use the pyBrain neural network library \cite{schaul2010}
to create an initial neural network that has a fixed topology.
By fixed topology, we mean that
the number of input neurons,
output neurons,
hidden layers,
neurons, and number of connections remain constant.
The only parameters we plan to optimize
in order to generate self-replicating neural networks
are the weights of the connections between neurons.
Since a feedforward neural network
can have many variations in its topology,
we have chosen a simple structure that makes the most sense.

One major challenge is deciding what kind of input
to feed into the neural network and how to interpret the output
of the neural network as another neural net.
For simplicity,
we decided on giving our neural network a N-bit vector
of binary numbers as input.
If we sequence all the connections in the neural network,
we can associate the value of binary vector
with a corresponding connection.
For example, the binary vector 1001
can be associated with the 5th connection
in a small network with 8 connections in total.
Accordingly, the output of the neural network
is a single value
and it represents the weight
of the connection specified by the input vector.
Thus by feeding the neural network
a set of inputs that map to all the connections in the network,
we can generate weights for each of connections.
This means that the network is capable of generating another net
exactly like itself in topology,
but with possibly different connection weights. 

The next step is to come up with
a method to optimize the connection weights
of the original network such that child network generated
also has the same connection weight.
Given the lack of gradients in this problem,
we decide to rely on a black-box optimization approach
to optimize the connection weights of the parent network.
Thus we require a fitness function
that evaluates how close the neural network is
to generating a child network exactly like itself.
To compute a fitness value,
as seen in Fig.~\ref{pseudo},
we use the parent network
to compute the connection weights of the child network,
determine the error between the new connection weight
and its original value in the parent network,
and return the sum of all the errors.
We try a variety of different black-box optimization methods,
including genetic algorithms \cite{deb2002fast},
CMA-ES \cite{hansen2003reducing},
and NES \cite{wierstra2008natural}
to try to minimize the error
and maximize the fitness of the neural network.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.8\linewidth]{pseudo.png}
\end{center}
   \caption{Function for computing fitness of a neural network.
For each connection in the parent network,
we convert it into a binary vector,
feed it as input,
and get the connection weight
of the same connection in the child network.
We compare the difference
between the child and parent weights
and sum up all differences as error,
which is then inverted to become a fitness value.}
\label{pseudo}
\end{figure} 

\section{Experimental Results}
\label{results}

For our first experiment,
we test to see if feedforward neural networks
could replicate themselves using the method described
in section \ref{feedforward}.
For our feed-forward neural network topology,
we decide on having an input layer with 8 neurons,
2 hidden layers with 12 neurons each,
and finally an output layer with a single neuron.
This network structure seems to have a good balance
between complexity, number of inputs, and performance.
All layers are fully connected
and each neuron uses a sigmoid
as its activation function.
The sigmoid allows the neural network
to output any range of values between -1 and 1.
Accordingly, the connection weights
of all the neurons are all randomly initialized between -1 and 1.
We optimize using CMAES
as described in the methodology for 500 generations.

Fig.~\ref{result1} shows how error goes down
as a function of the number of generations
that black-box optimization is applied
to connection weights of the parent network.
As we can see by the blue curve,
the error starts off around 150 and smoothly converges
to a value of around 5 at the 500th generation mark.
To show that our optimization method
and fitness functions are robust,
we deliberately clamp a certain percentage
of the connection weights (10\%, 20\%, and 40\%),
which results in the other curves
shown in Fig.~\ref{result1}.
While the final errors are not as good
as when none of the weights are clamped,
significant reductions in errors are still present.

However, while optimizing the connections
of the parent network results in a child network
that is similar to the parent,
the connection weights themselves do not seem
to be very interesting.
Fig.~\ref{histogram}(a) and (b) shows a histogram
of the connection weights
of both the optimized parent network
and the resulting child network respectively.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=1.0\linewidth]{result1.png}
\end{center}
   \caption{This chart shows error (as defined in section \ref{feedforward}) as a function of number of generations. The legend shows the percentage of connections that are clamped (fixed so that they do not change).}
\label{result1}
\end{figure}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=1.0\linewidth]{result1.png}
\end{center}
   \caption{Add in later.}
\label{histogram}
\end{figure}

Experimentally, we found that using CMA-ES resulted in the most improvement in fitness every generation.

\section{Conclusion. Future Work.}
\label{conclusion}

As we can see, the connections in both networks
have roughly same weights
at around roughly the 0.10 to 0.15 range.
From this, we can conclude that
if all the weights in a neural network are roughly the same,
the output of the neural network
is also roughly the same as an average
of all the weight values.
Thus the question arises whether it is
possible to create self-replicating networks
where the weights have interesting distributions. 

\renewcommand{\refname}{\section{References}}
\bibliography{selfrep}
\bibliographystyle{plain}

\end{document}
